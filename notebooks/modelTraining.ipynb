{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1752250153383,
     "user": {
      "displayName": "Raam Prekash",
      "userId": "03210573549722720918"
     },
     "user_tz": -330
    },
    "id": "zo10v9PmLws8",
    "outputId": "5bfc44d9-fe01-4745-d4e2-cc2529cd4bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz6pItiNMxuZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reference Path and Load Pre Processed Data\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/bert-sentiment-analysis/data/processed/train_processed.csv')\n",
    "val_df = pd.read_csv('/content/drive/MyDrive/bert-sentiment-analysis/data/processed/val_processed.csv')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/bert-sentiment-analysis/data/processed/test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1752250104000,
     "user": {
      "displayName": "Raam Prekash",
      "userId": "03210573549722720918"
     },
     "user_tz": -330
    },
    "id": "jVlJh1MhMZxK",
    "outputId": "7611b654-0bc6-403c-fa76-ef0124b5c9a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load base pretrained BERT model and classification head (3 labels - negative, neutral, positive)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 22506,
     "status": "ok",
     "timestamp": 1752249816190,
     "user": {
      "displayName": "Raam Prekash",
      "userId": "03210573549722720918"
     },
     "user_tz": -330
    },
    "id": "7LIOrNEVOcC9",
    "outputId": "5c26bf75-af76-49bc-f0b7-4ed9a477af57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.1\n",
      "    Uninstalling transformers-4.53.1:\n",
      "      Successfully uninstalled transformers-4.53.1\n",
      "Successfully installed transformers-4.53.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ff1dd67076164844ab3d40f1088318f5",
       "pip_warning": {
        "packages": [
         "transformers"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upgrade transformer to latest version\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4f88ebd50bbe4f548930201dd7d86bf4",
      "c92c0a47abea441197df240703e4c8fa",
      "1fe0ca52161147ea865d80da5e278c47",
      "6088590ad31f48728a6cf18e5d2585d5",
      "fc061025665b492aa0f96457f3a0dbb8",
      "2a3519100ff34ed0a4487e87be4e828c",
      "d44fb9b93cc143d2a8d187152b03be04",
      "11ba481e7d1f42409ac0950c007b56e0",
      "65d748526e2543c5918f43891f7f7c71",
      "e2623223fb95442194b761903ca32d79",
      "9ea249834dd14b4c9bdcc98b22f87981",
      "fbd2e74a81d3423aa61e7f0b8e65da07",
      "c4facaea141d4ae5a4f6ed8c7c8cf57e",
      "b56ca5624b0f4e90ab472b0d457f04b6",
      "34809ef564134174894e541a80cea1e1",
      "b85b529873b8423c90444e5dcf204dfa",
      "e4831e9fa5394a3a870e6844c793c3ae",
      "d68a59f335bc4f6b8084eb916811b089",
      "d2c6ee3c24ac4ca482e6c3b081b60b79",
      "ef4f296d69734dfbaf59a125279361f7",
      "9462d5593fbb407b8430d4c71578b81e",
      "c1770df92576460b8a5dfc69d06c4779",
      "a09aa74b2d354b01ac5fb48b4844b23c",
      "068f7f6157da42f1af6b4049ca87cfe2",
      "143b6872347543158c6584fd05f55128",
      "bd74a96b1fa941b8aea68b405db5075d",
      "0cdca7925da946e7af024ef1497ffe19",
      "944ff545c006480db3a1f1e492a74a4a",
      "04b8403ecb2648529c0831b796ccd311",
      "4c797ee2f7cd446cae8659e15176649f",
      "235769f8b3ff4b64bb708ee90dc92ca1",
      "0a9c10ad23ef449dbe6ac4e4409176bb",
      "cf6167e10ee5408986b464c60a187753"
     ]
    },
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1752250439892,
     "user": {
      "displayName": "Raam Prekash",
      "userId": "03210573549722720918"
     },
     "user_tz": -330
    },
    "id": "p5kZXF9SQ8QH",
    "outputId": "19d798ee-2127-466d-f89c-50c0a47f04c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f88ebd50bbe4f548930201dd7d86bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd2e74a81d3423aa61e7f0b8e65da07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09aa74b2d354b01ac5fb48b4844b23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-96DQEJQnNt"
   },
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(train_df['cleaned_sentence'])\n",
    "val_encodings = tokenize_texts(val_df['cleaned_sentence'])\n",
    "test_encodings = tokenize_texts(test_df['cleaned_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWEQkoEPRc29"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Tokenization of your text data.\n",
    "2. Conversion of labels to tensors\n",
    "3. Creation of PyTorch Dataset objects\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Step 1: Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 2: Define tokenization function\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Step 3: Tokenize your datasets\n",
    "train_encodings = tokenize_texts(train_df['cleaned_sentence'])\n",
    "val_encodings = tokenize_texts(val_df['cleaned_sentence'])\n",
    "test_encodings = tokenize_texts(test_df['cleaned_sentence'])\n",
    "\n",
    "# Step 4: Convert labels to torch tensors\n",
    "train_labels = torch.tensor(train_df['label'].values)\n",
    "val_labels = torch.tensor(val_df['label'].values)\n",
    "test_labels = torch.tensor(test_df['label'].values)\n",
    "\n",
    "# Step 5: Create Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Step 6: Create Dataset objects\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzrkX6SZSv36"
   },
   "outputs": [],
   "source": [
    "# Set up optimizer and learning rate scheduler\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_training_steps = len(train_dataset) // batch_size * num_epochs\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP51nN3KCM8jUVqTJ9VQJ5K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
