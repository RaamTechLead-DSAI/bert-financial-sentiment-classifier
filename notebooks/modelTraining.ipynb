{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zo10v9PmLws8",
    "outputId": "0e963494-f969-4f02-aa2c-c67164ccc9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access and save files persistently across sessions\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "okk4K2zxE6Rt"
   },
   "outputs": [],
   "source": [
    "# Disable Weights & Biases (wandb) to allow training without requiring an API key or logging\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c3B2NuZnviCZ"
   },
   "outputs": [],
   "source": [
    "# Load the pre-processed datasets required for model training and evaluation\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/bert-sentiment-analysis/data/processed/train_processed.csv')\n",
    "val_df = pd.read_csv('/content/drive/MyDrive/bert-sentiment-analysis/data/processed/val_processed.csv')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/bert-sentiment-analysis/data/processed/test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "7f368afc532943e8a3badd9816924f95",
      "3d62b8e994cc491594fd97a6724da7ee",
      "fd35270f455541988638d41b88d3d91b",
      "d03cdf55faa84faa85b3ed5f2279034a",
      "e1666c0931f043068ee3be5e1eb8151a",
      "99adcde098a24ec7b87b7758d273b563",
      "915a2e621265417c8a5ec929f7e6a8f9",
      "7d9b1fce25694f9fb9e74494cfc346f2",
      "a8f89bbd6cfe47d1b5589828de43ab6c",
      "59c123cfdb28497198e8ac60994f12af",
      "8d235f3f13474970ba47c60355202a4d",
      "2674e41e8a10489db154791f03a294e9",
      "fa7df3f459104305bb1c36df03806299",
      "73a76e91a43e4a8ab8f32236c68e458d",
      "22053174dd54472ca8a6e84ee9869d9c",
      "e2765a662e5a4796a2521c20ee3dbb86",
      "67a5f2d119e049378d7c20447f5f04af",
      "c8bb1ba4cd644ac9a5d938a4717f5cd0",
      "1c2e06ebbb914113ba44aefab6ef7e48",
      "6712a15ca10046b6be8fee8ea37f928f",
      "b68ce67d480d463fbb01dd161bc0aa1d",
      "e83f66d91cc44f6a8a0e31bee1d87b69",
      "453999a41ae24b179cd90f33af0cd163",
      "6e3b315b17c142e0818951fae4fbe993",
      "105c61a7831f4f92bdbcd3a84d85b543",
      "1d0c556f0e5b4c768e23232db16338d3",
      "b632ec6e2de141138725f4c7ccfd9ff0",
      "e2511ef2a31f401fb86bf58c52c7a936",
      "09a842abae8c4361bb00f958bd1b16d5",
      "cd3f323887fc4dfcaa614a3c58d9ed3f",
      "ae04957a1abc4a299eb2b93de206d291",
      "80d2d00fcb19428983b84b8b2d829c27",
      "b9eb07dafeb942648d0f5ae947991862",
      "2da4237459424f31a25e888336fa0cf4",
      "fb7b260ce91c428e9feb73871a857289",
      "aa89714da9f643489270d33172ebc6d2",
      "1179d571e45540d1ba802065226db8f5",
      "2c498ede93244f2f8f4ac3b80e705ae2",
      "822efa57ea3f48759b7d2334355b4268",
      "0eaa394b32be4c2c87f2b924a12db992",
      "975e53c43eb84711a071fad359e0771f",
      "fcd81394d3d84471816d3a6e3d4d5d12",
      "f2ad41bb69c64625a0a6d638f47de748",
      "d54c0a03cca24b7d8415cdaf7f3e103c"
     ]
    },
    "id": "p5kZXF9SQ8QH",
    "outputId": "969f0aa2-0495-484b-ceea-9c0931494efd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f368afc532943e8a3badd9816924f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2674e41e8a10489db154791f03a294e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453999a41ae24b179cd90f33af0cd163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da4237459424f31a25e888336fa0cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the pretrained BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q-96DQEJQnNt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenizes a list of texts using the pre-defined tokenizer.\n",
    "\n",
    "Args:\n",
    "    texts (list or pd.Series): List or Series of sentences to tokenize.\n",
    "    max_length (int): Maximum sequence length after padding/truncation.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of tokenized outputs as PyTorch tensors.\n",
    "'''\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length', # pad shorter texts to max_length\n",
    "        truncation=True, # truncate longer texts\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt' # return PyTorch tensors\n",
    "    )\n",
    "# Tokenize the cleaned sentences from each dataset split for model input\n",
    "train_encodings = tokenize_texts(train_df['cleaned_sentence'])\n",
    "val_encodings = tokenize_texts(val_df['cleaned_sentence'])\n",
    "test_encodings = tokenize_texts(test_df['cleaned_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MCHfdeqnxi0y"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Convert label columns from each dataset split into PyTorch tensors\n",
    "For compatibility with model training and loss calculation\n",
    "'''\n",
    "import torch\n",
    "\n",
    "train_labels = torch.tensor(train_df['label'].values)\n",
    "val_labels = torch.tensor(val_df['label'].values)\n",
    "test_labels = torch.tensor(test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeHED5Kex60-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# Custom dataset class for sentiment analysis data\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset to handle input encodings and labels for sentiment analysis.\n",
    "    Provides data in a format compatible with DataLoader for batching during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "     # Retrieve encodings and label for a single example at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of examples in the dataset\n",
    "        return len(self.labels)\n",
    "\n",
    "# Instantiate Dataset objects for training and validation splits\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "c4e9c03f216441a6b1909959c3827de4",
      "6dce5771b3af47908b70d66bed4ec0fa",
      "c6da5ebdf7df4673b3ddfea29db05397",
      "ecc9f6e9e5e743f7b76b3a140c016cd2",
      "26725c64209d43f98004882be94bff75",
      "3da35e30ae0d4d269aa803b6487283c4",
      "7970bb9ffacf4876a05143d16d1138d0",
      "73f1204c638d4a038c99286234bbdf60",
      "5f1a8a5ec9594a279629835b4b53ff30",
      "15c8581ab722459fbbec0cf42da5235d",
      "3ab3b56b5a5940a398abc65c306c7964"
     ]
    },
    "id": "jVlJh1MhMZxK",
    "outputId": "48fda0c7-aa23-4b31-bbea-7d651b813b89"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e9c03f216441a6b1909959c3827de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the pretrained BERT base model with a classification head for 3 sentiment classes: negative, neutral, positive)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1zTUDZ-yUK5"
   },
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler for training\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "# AdamW optimizer is commonly used with transformer models for weight decay regularization\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Calculate total number of training steps (batches * epochs)\n",
    "num_training_steps = len(train_dataset) // batch_size * num_epochs\n",
    "\n",
    "# Linear learning rate scheduler with optional warmup steps (none here)\n",
    "# Gradually decreases the learning rate from the initial value to zero over training\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "7LIOrNEVOcC9",
    "outputId": "5e847f9d-98b1-41af-c62e-2cc987ac688f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.1\n",
      "    Uninstalling transformers-4.53.1:\n",
      "      Successfully uninstalled transformers-4.53.1\n",
      "Successfully installed transformers-4.53.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "560ff67d718f4655861978212b7566f2",
       "pip_warning": {
        "packages": [
         "transformers"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upgrade transformer to latest version\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNBbiqzxspT-",
    "outputId": "178b0101-9fb7-4545-e966-f0d06acd9db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Confirm the version of transformer\n",
    "import transformers\n",
    "print(transformers.__version__) # Print transformers library version\n",
    "print(transformers.__file__) # Print path to the transformers module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqqSv2ti2mBZ",
    "outputId": "c449ea68-14dc-4e6b-9acb-00a5c5fb9f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/runs/Jul12_09-08-47_32e9c15eb1ed,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training configuration and hyperparameters for the Trainer API\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy='epoch'\n",
    ")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXJs-rWM4p68",
    "outputId": "52bff9ce-ab00-47fe-b39e-8d3683ff8fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\n",
      "transformers.training_args\n",
      "No __file__ attribute\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Print the installed transformers library version and file path\n",
    "print(transformers.__version__)\n",
    "print(transformers.__file__)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Print the module where TrainingArguments is defined and its file path (if available)\n",
    "print(TrainingArguments.__module__)\n",
    "print(TrainingArguments.__file__ if hasattr(TrainingArguments, '__file__') else 'No __file__ attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0iJ_Kqm45-y",
    "outputId": "58e1f388-f4e0-44e8-98fb-c6d5e3757241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Print the signature of the TrainingArguments constructor\n",
    "print(inspect.signature(TrainingArguments.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "dEw6hqkNzYAd",
    "outputId": "046b1a3b-b07d-40e6-912b-868dcd411c01"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipython-input-19-3704577386.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='696' max='792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [696/792 4:41:57 < 39:00, 0.04 it/s, Epoch 2.63/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>0.755705</td>\n",
       "      <td>0.768427</td>\n",
       "      <td>0.813518</td>\n",
       "      <td>0.755705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.766443</td>\n",
       "      <td>0.775856</td>\n",
       "      <td>0.792335</td>\n",
       "      <td>0.766443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='792' max='792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [792/792 5:25:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>0.755705</td>\n",
       "      <td>0.768427</td>\n",
       "      <td>0.813518</td>\n",
       "      <td>0.755705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.766443</td>\n",
       "      <td>0.775856</td>\n",
       "      <td>0.792335</td>\n",
       "      <td>0.766443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.645472</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>0.762035</td>\n",
       "      <td>0.767050</td>\n",
       "      <td>0.758389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=792, training_loss=0.3971877760357327, metrics={'train_runtime': 19541.3894, 'train_samples_per_second': 0.648, 'train_steps_per_second': 0.041, 'total_flos': 832753967109120.0, 'train_loss': 0.3971877760357327, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define training arguments for the Hugging Face Trainer API\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/bert-financial-sentiment-classifier/results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # training batch size per device\n",
    "    per_device_eval_batch_size=16,   # evaluation batch size\n",
    "    eval_strategy='epoch',           # evaluate at the end of each epoch\n",
    "    save_strategy='epoch',           # save checkpoint every epoch\n",
    "    learning_rate=5e-5,              # learning rate\n",
    "    weight_decay=0.01,               # weight decay\n",
    "    logging_dir='/content/drive/MyDrive/bert-financial-sentiment-classifier/logs',            # logging directory\n",
    "    logging_steps=50,                # log every 50 steps\n",
    "    load_best_model_at_end=True,     # load best model at end of training\n",
    "    metric_for_best_model='accuracy' # metric to use for best model selection\n",
    ")\n",
    "\n",
    "# Define metric computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, and F1-score for model evaluation.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing logits (model outputs) and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with 'accuracy', 'f1', 'precision', and 'recall' scores.\n",
    "    \"\"\"\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate precision, recall, f1-score with weighted average (handles class imbalance)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    # Return all metrics in a dictionary format expected by Trainer\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Initialize the Hugging Face Trainer with model, datasets, tokenizer, and evaluation metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o0ShicXQaMC",
    "outputId": "f1be1557-0f83-4ba2-8254-adf91c126f29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/bert-financial-sentiment-classifier/model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/bert-financial-sentiment-classifier/model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/bert-financial-sentiment-classifier/model/vocab.txt',\n",
       " '/content/drive/MyDrive/bert-financial-sentiment-classifier/model/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer to the specified directory\n",
    "# This allows loading the trained model/tokenizer later for inference or further training\n",
    "model.save_pretrained('/content/drive/MyDrive/bert-financial-sentiment-classifier/model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/bert-financial-sentiment-classifier/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIIKRceqRC6N",
    "outputId": "d82c34a4-52bf-4ae0-a674-bf170afbbf80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Contents: ['config.json', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = '/content/drive/MyDrive/bert-financial-sentiment-classifier/model'\n",
    "\n",
    "# Check if the saved model directory exists and list its contents\n",
    "print(\"Exists:\", os.path.exists(model_dir))\n",
    "print(\"Contents:\", os.listdir(model_dir) if os.path.exists(model_dir) else \"Folder not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "_IwUtGLiUkaE",
    "outputId": "3a0b94e7-03bb-4851-dcd4-004140ed750c"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cleaned_sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cleaned_sentence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-24-87803634.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Tokenize test texts (make sure tokenizer is initialized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m test_encodings = tokenizer(\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cleaned_sentence'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load processed test data CSV\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/bert-financial-sentiment-classifier/data/processed/test_processed.csv')\n",
    "\n",
    "\n",
    "# Tokenize test texts (make sure tokenizer is initialized)\n",
    "test_encodings = tokenizer(\n",
    "    list(test_df['cleaned_sentence']),\n",
    "    padding='max_length', # Pad all sequences to max_length\n",
    "    truncation=True,      # Truncate sequences longer than max_length\n",
    "    max_length=128,\n",
    "    return_tensors='pt'   # Return PyTorch tensors for model input\n",
    ")\n",
    "\n",
    "# Convert test labels to PyTorch tensor for compatibility with the model\n",
    "test_labels = torch.tensor(test_df['label'].values)\n",
    "\n",
    "# Define dataset class to wrap encodings and labels for PyTorch DataLoader compatibility\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Instantiate the test dataset\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# Evaluate the fine-tuned model on the test dataset using the Hugging Face Trainer\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsWwL3FrVz5n",
    "outputId": "fc3912fb-499b-4c75-a9a7-5d42e218953d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['Sentence', 'Sentiment', 'label'], dtype='object')\n",
      "                                            Sentence Sentiment  label\n",
      "0  The number of bodily injury cases quadrupled i...  negative      0\n",
      "1  Net sales decreased to EUR 91.6 mn from EUR 10...   neutral      1\n",
      "2   $aapl high of day just hit. Back at it tomorrow.  positive      2\n",
      "3  According to CEO Kai Telanne , the company 's ...  positive      2\n",
      "4  Finland 's dominating rail company VR is plann...   neutral      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed test dataset from CSV\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/bert-financial-sentiment-classifier/data/processed/test_processed.csv')\n",
    "\n",
    "# Display the column names and first few rows to verify data structure and content\n",
    "print(\"Columns:\", test_df.columns)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3V_wR3HWqjH",
    "outputId": "caa06af4-478f-4a4c-8b28-e4f1c16c6981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence Sentiment  label  \\\n",
      "0  The number of bodily injury cases quadrupled i...  negative      0   \n",
      "1  Net sales decreased to EUR 91.6 mn from EUR 10...   neutral      1   \n",
      "2   $aapl high of day just hit. Back at it tomorrow.  positive      2   \n",
      "3  According to CEO Kai Telanne , the company 's ...  positive      2   \n",
      "4  Finland 's dominating rail company VR is plann...   neutral      1   \n",
      "\n",
      "                                    cleaned_sentence  \n",
      "0    The number of bodily injury cases quadrupled in  \n",
      "1  Net sales decreased to EUR  mn from EUR mn in ...  \n",
      "2           high of day just hit Back at it tomorrow  \n",
      "3  According to CEO Kai Telanne  the company s ne...  \n",
      "4  Finland s dominating rail company VR is planni...  \n"
     ]
    }
   ],
   "source": [
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUFuGjj6WRK5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean input text by:\n",
    "    - Removing URLs (http, https, www)\n",
    "    - Removing stock ticker symbols starting with '$'\n",
    "    - Removing non-alphabetic characters (keeping spaces)\n",
    "    - Stripping leading/trailing whitespace\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Check if 'cleaned_sentence' column exists\n",
    "if 'cleaned_sentence' not in test_df.columns:\n",
    "    test_df['cleaned_sentence'] = test_df['Sentence'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXA_g4NAXTKJ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenize the cleaned test sentences using the pretrained tokenizer\n",
    "\t- Pads all sequences to max_length (128)\n",
    "\t- Truncates sequences longer than max_length\n",
    "\t- Returns PyTorch tensors for input to the model\n",
    "'''\n",
    "test_encodings = tokenizer(\n",
    "    list(test_df['cleaned_sentence']),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8clV-VF5XjNO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert test labels to PyTorch tensor for compatibility with the model during evaluation\n",
    "test_labels = torch.tensor(test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rFjYS0BXwiM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling tokenized inputs and labels for sentiment analysis.\n",
    "    Enables easy batching and iteration over the dataset during training or evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): Tokenized inputs (e.g., input_ids, attention_mask) as tensors.\n",
    "            labels (torch.Tensor): Corresponding labels tensor.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single data point with its tokenized inputs and label.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data point to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing input tensors and label for the given index.\n",
    "        \"\"\"\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSk4JppPX05B"
   },
   "outputs": [],
   "source": [
    "# Create the test dataset object using the tokenized inputs and label tensors\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "XxAFsPWyX7pl",
    "outputId": "35e853cb-974b-4814-f777-50800c9bdaae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 06:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5291472673416138, 'eval_accuracy': 0.8027366020524516, 'eval_f1': 0.8093768490883736, 'eval_precision': 0.823593416797985, 'eval_recall': 0.8027366020524516, 'eval_runtime': 386.8626, 'eval_samples_per_second': 2.267, 'eval_steps_per_second': 0.142, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the test dataset using the Hugging Face Trainer\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print evaluation metrics such as accuracy, precision, recall, and F1-score\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
